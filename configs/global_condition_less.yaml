model:
  base_learning_rate: 1e-7
  target: ldm.models.diffusion.ddpm.AutoDM_GlobalCondition
  params:
    base_learning_rate: 1e-7
    linear_start: 0.0015
    linear_end: 0.0155
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    loss_type: l1
    image_size: [16,32]
    channels: 4
    cond_stage_trainable: True
    scale_by_std: True
    monitor: 'train/loss_simple'
    unet_trainable: True
    learn_logvar: True
    downsample_img_size: [16,32]
    init_from_video_model: False
    movie_len: 5
    predict: False
    cond_stage_config: 
      target: ldm.models.autoencoder.FrozenCLIPTextEmbedder
    scheduler_config: #10000 warmup steps
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [10000]
        cycle_lengths: [10000000000000]
        f_start: [1.e-6]
        f_max: [1.]
        f_min: [1.]
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: [16,32]
        in_channels: 4
        out_channels: 4
        num_classes: 2
        model_channels: 320
        attention_resolutions: [1,2,4] #32 16 8 4
        num_res_blocks: 2
        channel_mult: [1,2,4,4] #32,16,8,4,2
        num_heads: 8
        use_scale_shift_norm: False
        resblock_updown: False
        context_dim: 768
        use_checkpoint: False
        use_spatial_transformer: True
        ckpt_path: stable_diffusion/sd-v1-4.ckpt
        # ckpt_path: logs/2024-08-06T07-11-46_global_condition/checkpoints/last.ckpt
        # ckpt_path: logs/2024-08-04T21-52-13_global_condition/checkpoints/last.ckpt
        movie_len: 5
        height: 16
        width: 32
        obj_dims: 768
        outpadding: [[0,0],[0,0],[0,0]]
        # ignore_keys: ['first_stage_model','hdmap_encoder']
        # modify_keys: ['box_encoder']
        # ignore_keys: []
        # modify_keys: []
        ignore_keys: ['model.diffusion_model.input_blocks.0.0']

      
    global_condition_config:
      target: ldm.models.condition.GlobalCondition
      params:
        image_config:
          target: ldm.models.autoencoder.AutoencoderKL_Diffusion
          params:
            embed_dim: 4
            monitor: "val/rec_loss"
            ckpt_path: stable_diffusion/kl-f8.ckpt
            trainable: False
            ddconfig:
              double_z: True
              z_channels: 4
              resolution: 256
              in_channels: 3
              out_ch: 3
              ch: 128
              ch_mult: [1,2,4,4] #num_down=len(ch_mult)-1
              num_res_blocks: 2
              attn_resolutions: []
              dropout: 0.0
              num_groups: 32
              movie_len: 5
            lossconfig:
              target: torch.nn.Identity

        lidar_config:
          target: ldm.models.autoencoder.Autoencoder_Lidar2
          params:
            embed_dim: 4
            monitor: "val/rec_loss"
            ckpt_path: logs/2024-08-02T21-23-17_autoencoder_lidar/checkpoints/last.ckpt
            trainable: False
            image_key: 'range_image'
            ignore_keys: []
            ddconfig:
              double_z: True
              z_channels: 4
              resolution: 256
              in_channels: 3
              out_ch: 3
              ch: 128
              ch_mult: [1,2,4,4] #num_down=len(ch_mult)-1
              num_res_blocks: 2
              attn_resolutions: [16]
              dropout: 0.0
              num_groups: 32
              
            lossconfig:
              target: ldm.modules.losses.LPIPSWithDiscriminator
              params:
                disc_start: 1
                kl_weight: 0.000001
                disc_weight: 0.5
        box_config:
          target: ldm.models.autoencoder.Fourier_Embedding
          params:
            in_channels: 16
            ch: [256,512,768]
            context_dims: 768
            sigma: 1
            trainable: False
        # action_encoder_config:
        #   target: ldm.models.actionformer.ActionEncoder
        #   params:
        #     in_channels: 7
        #     ch: [32,128]
        #     trainable: True

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 8
    wrap: False
    num_workers: 1
    train:
      target: data.dataloader_predict_video.dataloader
      params:
        cfg:
          version: advanced_12Hz_trainval
          dataroot: /storage/group/4dvlab/datasets/nuScenes/
          img_size: [256,128]
          nusc_canbus_frequency: 50
          camera_frequency: 2
        num_boxes: 70
        movie_len: 5
        split_name: train
        collect_condition: ['reference_image','range_image','text']
  
    validation:
      target: data.dataloader_predict_video.dataloader
      params:
        cfg:
          version: advanced_12Hz_trainval
          dataroot: /storage/group/4dvlab/datasets/nuScenes/
          img_size: [256,128]
          nusc_canbus_frequency: 50
          camera_frequency: 2
        num_boxes: 70
        movie_len: 5
        split_name: val
        collect_condition: ['reference_image','range_image','text']


lightning:
  callbacks:
    video_logger:
      target: main.VideoLogger
      params:
        batch_frequency: 2000
        max_videos: 8
        fps: 5
        increase_log_steps: False

  trainer:
    benchmark: True
    num_nodes: 1
    gpus: 0,1,2,3,

    